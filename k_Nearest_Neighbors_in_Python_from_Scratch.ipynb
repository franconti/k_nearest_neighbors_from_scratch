{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "k- nearest neighbors classifier is a model that consist only of storing the training set and make prediction for a new data point finding the point in the training set that is closest to the new point.Then it assigns the label of this training point to the new data point. It relies on a chosen distance metric (e.g., Euclidean, Manhattan) to find nearest neighbors.\n",
        "\n",
        "The k in k-nearest neighbors signifies that instead of using only the closest neighbor to the new data point, the model can be fixed with any number \"k\" of neighbors in the training.\n",
        "\n",
        "\n",
        "The goal of this post is to build a k-Nearest Neighbors model from scratch.  The development process will involve the following steps:\n",
        "\n",
        "1 - Calculate distance.\n",
        "\n",
        "2 - Get nearest neighbors.\n",
        "\n",
        "3 - Make predictions\n",
        "\n",
        "\n",
        "For categorical data, where the outcome is represented by distinct categories, this algorithm determines the most frequent class among the k nearest neighbors.\n",
        "\n",
        "For continuous data, where the outcome is a numerical value, two alternative prediction methods can be employed:\n",
        "\n",
        "Inverse Distance Weighting: In this method, closer neighbors are assigned higher weights, giving them more influence on the prediction. The weights are inversely proportional to the distance from the new data point.\n",
        "\n",
        "Exponential Distance Weighting: This method assigns exponentially decaying weights to neighbors as their distance from the new data point increases.\n"
      ],
      "metadata": {
        "id": "Qlan2Y4ReE7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data set\n",
        "\n",
        "For classification, we will utilize the Iris dataset from scikit-learn. This dataset comprises measurements of petal and sepal length for three different types of irises: Setosa, Versicolour, and Virginica. The data is stored in a 150x4 NumPy array.\n",
        "\n",
        "For regression, we will employ the Diabetes dataset from scikit-learn. The target variable in this dataset consists of integer values ranging from 25 to 346.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OhxK1YZlsK53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "14wgmAoyiAs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "ziDsO1AaiDKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7n15RaY7aewr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that randomly divides an array or matrix into train and test subsets. This step could be accomplished using the train_test_split function from the sklearn.model_selection module in a single line of code.\n",
        "\n",
        "However, implementing it from scratch provides a deeper understanding of the data splitting process and allows for customization based on specific requirements.\n",
        "\n"
      ],
      "metadata": {
        "id": "D2YytKK-j3KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_random_split(p_train,X,Y):\n",
        "    assert 0 < p_train < 1, 'ratio must be between zero and one'\n",
        "    data_len = len(X) #data lenght\n",
        "    train_len = round(p_train*data_len) #get integer number\n",
        "    perm = np.random.permutation(data_len) #generates random numbers\n",
        "    train_pos = perm[:train_len] #take random numbers from 0 to desire train lenght\n",
        "    test_pos = perm[train_len:] #same for test\n",
        "    return X[train_pos], Y[train_pos], X[test_pos], Y[test_pos] #ste values\n"
      ],
      "metadata": {
        "id": "T4c_n93qpNvB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object oriented k-nearest code"
      ],
      "metadata": {
        "id": "rBfj2hPA6l8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class KNN:\n",
        "\n",
        "    def __init__(self,x_train,y_train,k,classification_mode=True,\\\n",
        "                 weight_function='ones'):\n",
        "        self.X_train = x_train\n",
        "        self.Y_train = y_train\n",
        "        self.k = k\n",
        "        self.classification_mode = classification_mode\n",
        "        self.weight_function = weight_function\n",
        "\n",
        "\n",
        "    def _distance(training_data, point):\n",
        "      #calculates the Euclidean distance between each data point\n",
        "        return np.linalg.norm(training_data - point,axis=1)\n",
        "\n",
        "    def _k_neighbours(self,point):\n",
        "       #all the stuff declare at self and the new external point\n",
        "        distances = KNN._distance(self.X_train,point) #get distance all data set\n",
        "        order_of_distances = distances.argsort() #sort distances in ascending order\n",
        "        sorted_distances = distances[order_of_distances] #sort the actual distances along the indices\n",
        "        sorted_labels = self.Y_train[order_of_distances] #get the labels of the sorted points\n",
        "        return sorted_labels[:self.k], sorted_distances[:self.k] #return the k first labels and distances\n",
        "\n",
        "    def _majority_vote(self,y_labels,distances):\n",
        "\n",
        "        if self.classification_mode: #schecks if the KNN is used for classification\n",
        "            votes, freq = np.unique(y_labels, return_counts=True) #finds the unique values (classes)\n",
        "            most_common_val = votes[np.argmax(freq)] #finds the most frequent\n",
        "            prediction = most_common_val #extracts the corresponding class label\n",
        "\n",
        "        else: #regression (predicting continuous values) 3 mehtods\n",
        "            if self.weight_function == 'inverse': #gives closer neighbors more weight\n",
        "                eps = 0.1\n",
        "                weights = 1/(distances+eps)\n",
        "            elif self.weight_function == 'exponential': #assigns exponentially decaying weights based on distance,\n",
        "                weights = np.exp(-distances)\n",
        "            else:\n",
        "                weights = np.ones_like(distances) #uses equal weights for all neighbors.\n",
        "            weighted_avg = np.average(y_labels, weights=weights) #calculates a weighted average of the neighbor labels using the chosen weights.\n",
        "            prediction = weighted_avg #sets the predicted value\n",
        "        return prediction\n",
        "\n",
        "    def predict(self,point):\n",
        "      #Predicts the class or value for a new data point based on the majority vote of its k nearest neighbors.\n",
        "        sorted_labels, sorted_distances = self._k_neighbours(point)\n",
        "        return self._majority_vote(sorted_labels, sorted_distances)\n",
        "\n",
        "    def _predict_on_batch(self, X_test):\n",
        "       #make predictions for  X-test\n",
        "        return np.array([self.predict(test_row) for test_row in X_test])\n",
        "\n",
        "    def score(self,y_test,X_test,type_data): #compara resultado de Xtest con el valor real, y_teest\n",
        "        y_calculated = self._predict_on_batch(X_test) #lista de resultados calculados\n",
        "        assert len(y_test) == len(y_calculated),'labels and predictions must be the same length' #assert ensures the number of true labels and predicted labels are the same.\n",
        "\n",
        "        if self.classification_mode:\n",
        "          #This block handles the scenario where the model is predicting discrete classes (classification).\n",
        "\n",
        "            score = (y_test == y_calculated).sum()/len(y_test) #calculates the accuracy\n",
        "            print('Success percentage on '+type_data+' {:.2%}\\n'.format(score))\n",
        "\n",
        "        else:\n",
        "           #here it deals with regression problems where the model predicts continuous values\n",
        "\n",
        "            score = np.sqrt( ((y_test-y_calculated)**2).mean() ) #measure of the differences between values predicted by a model and the values observed in reality.\n",
        "            print('average error on '+type_data+' {:.4}\\n'.format(score))\n",
        "        # print(f'The score is {score}')\n",
        "        return score\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "8OR9U3i3jhqa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test code for clasification"
      ],
      "metadata": {
        "id": "9Hh_lCnQ7LFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS = True\n",
        "DIABETES = False"
      ],
      "metadata": {
        "id": "NmQumg7Zldm9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    if IRIS:\n",
        "        dataset = datasets.load_iris()\n",
        "        classify = True\n",
        "    if DIABETES:\n",
        "        dataset = datasets.load_diabetes()\n",
        "        classify = False\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = train_test_random_split(0.7,dataset.data, dataset.target)\n",
        "\n",
        "    knn = KNN(X_train, Y_train, 3,classification_mode=classify,weight_function='exponential')\n",
        "\n",
        "    score_train = knn.score(Y_train,X_train,'train')\n",
        "    score_test = knn.score(Y_test,X_test,'test')\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDuVcv-llTJ_",
        "outputId": "717623ca-f02f-4bee-d97d-b6f7e858e8a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success percentage on train 96.19%\n",
            "\n",
            "Success percentage on test 95.56%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using sklearn library"
      ],
      "metadata": {
        "id": "uvwPv7AQQtgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dataset = datasets.load_iris()\n",
        "X_train, Y_train, X_test, Y_test = train_test_random_split(0.7,dataset.data, dataset.target)\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, Y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXj3y5ee-CdT",
        "outputId": "82fe665e-c41b-48c9-980a-30567a6ef048"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We arrive to the same Accuracy than the one obtained in clasification mode.**"
      ],
      "metadata": {
        "id": "lRPUQyrqDOmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing to regresion\n"
      ],
      "metadata": {
        "id": "btTNzP2JwTTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS = False\n",
        "DIABETES = True"
      ],
      "metadata": {
        "id": "Wlq1ObqlwM-X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting weight_function='exponential'"
      ],
      "metadata": {
        "id": "t9za2wzTxIay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    if IRIS:\n",
        "        dataset = datasets.load_iris()\n",
        "        classify = True\n",
        "    if DIABETES:\n",
        "        dataset = datasets.load_diabetes()\n",
        "        classify = False\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = train_test_random_split(0.7,dataset.data, dataset.target)\n",
        "\n",
        "    knn = KNN(X_train, Y_train, 3,classification_mode=classify,weight_function='exponential')\n",
        "\n",
        "    score_train = knn.score(Y_train,X_train,'train')\n",
        "    score_test = knn.score(Y_test,X_test,'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9_8YscGwscs",
        "outputId": "078a3a87-01da-4109-fc5c-9df0d2031ba6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average error on train 44.69\n",
            "\n",
            "average error on test 62.24\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weight_function='inverse'"
      ],
      "metadata": {
        "id": "--KAjPpVxZBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    if IRIS:\n",
        "        dataset = datasets.load_iris()\n",
        "        classify = True\n",
        "    if DIABETES:\n",
        "        dataset = datasets.load_diabetes()\n",
        "        classify = False\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = train_test_random_split(0.7,dataset.data, dataset.target)\n",
        "\n",
        "    knn = KNN(X_train, Y_train, 3,classification_mode=classify,weight_function='inverse')\n",
        "\n",
        "    score_train = knn.score(Y_train,X_train,'train')\n",
        "    score_test = knn.score(Y_test,X_test,'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZjJYCDxPtd",
        "outputId": "12492e78-95a6-4fa8-aae0-b13b819c17e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average error on train 37.19\n",
            "\n",
            "average error on test 54.96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the high error rates in both inverse and exponential weights, it can be concluded that the current model is not well-suited for making these types of predictions.\n",
        "\n",
        "In both cases, the error in prediction test is high. We can conclude that ithis is not a good model to make this predictions."
      ],
      "metadata": {
        "id": "3_-EXkyb9NXq"
      }
    }
  ]
}